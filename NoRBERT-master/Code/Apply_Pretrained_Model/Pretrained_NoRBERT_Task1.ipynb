{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Pretrained_NoRBERT_Task1.ipynb",
   "provenance": [],
   "private_outputs": true,
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IiDlHo-_CRac",
    "colab_type": "text"
   },
   "source": [
    "# Binary classification of functional and non-functional requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18lQiv4UBSdt",
    "colab_type": "text"
   },
   "source": [
    "This notebook can be used to load a pretrained model, trained on the task of binary classification of functional and non-functional requirements on the original Promise NFR dataset. It can be used to classify a given input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSq4Qamxdkz2",
    "colab_type": "text"
   },
   "source": [
    "Note: some cells are hidden and only the title is shown. To display the code, double-click the cell to switch the display mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-1y5KaxYVmP",
    "colab_type": "text"
   },
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GQN5E-FoUqsv",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "#@title Install dependencies {display-mode: \"form\"}\n",
    "!pip install pytorch-transformers fastprogress\n",
    "!pip install fastai==1.0.57"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-transformers in e:\\program files (x86)\\conda\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: fastprogress in e:\\program files (x86)\\conda\\lib\\site-packages (1.0.3)\n",
      "Requirement already satisfied: sacremoses in e:\\program files (x86)\\conda\\lib\\site-packages (from pytorch-transformers) (0.0.53)\n",
      "Requirement already satisfied: boto3 in e:\\program files (x86)\\conda\\lib\\site-packages (from pytorch-transformers) (1.24.28)\n",
      "Requirement already satisfied: sentencepiece in e:\\program files (x86)\\conda\\lib\\site-packages (from pytorch-transformers) (0.1.97)\n",
      "Requirement already satisfied: torch>=1.0.0 in e:\\program files (x86)\\conda\\lib\\site-packages (from pytorch-transformers) (1.13.1)\n",
      "Requirement already satisfied: requests in e:\\program files (x86)\\conda\\lib\\site-packages (from pytorch-transformers) (2.28.1)\n",
      "Requirement already satisfied: numpy in e:\\program files (x86)\\conda\\lib\\site-packages (from pytorch-transformers) (1.21.5)\n",
      "Requirement already satisfied: regex in e:\\program files (x86)\\conda\\lib\\site-packages (from pytorch-transformers) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in e:\\program files (x86)\\conda\\lib\\site-packages (from pytorch-transformers) (4.64.1)\n",
      "Requirement already satisfied: typing_extensions in e:\\program files (x86)\\conda\\lib\\site-packages (from torch>=1.0.0->pytorch-transformers) (4.3.0)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in e:\\program files (x86)\\conda\\lib\\site-packages (from boto3->pytorch-transformers) (0.6.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in e:\\program files (x86)\\conda\\lib\\site-packages (from boto3->pytorch-transformers) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.28 in e:\\program files (x86)\\conda\\lib\\site-packages (from boto3->pytorch-transformers) (1.27.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\program files (x86)\\conda\\lib\\site-packages (from requests->pytorch-transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in e:\\program files (x86)\\conda\\lib\\site-packages (from requests->pytorch-transformers) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\program files (x86)\\conda\\lib\\site-packages (from requests->pytorch-transformers) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in e:\\program files (x86)\\conda\\lib\\site-packages (from requests->pytorch-transformers) (2.0.4)\n",
      "Requirement already satisfied: six in e:\\program files (x86)\\conda\\lib\\site-packages (from sacremoses->pytorch-transformers) (1.16.0)\n",
      "Requirement already satisfied: click in e:\\program files (x86)\\conda\\lib\\site-packages (from sacremoses->pytorch-transformers) (8.0.4)\n",
      "Requirement already satisfied: joblib in e:\\program files (x86)\\conda\\lib\\site-packages (from sacremoses->pytorch-transformers) (1.1.0)\n",
      "Requirement already satisfied: colorama in e:\\program files (x86)\\conda\\lib\\site-packages (from tqdm->pytorch-transformers) (0.4.5)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in e:\\program files (x86)\\conda\\lib\\site-packages (from botocore<1.28.0,>=1.27.28->boto3->pytorch-transformers) (2.8.2)\n",
      "Requirement already satisfied: fastai==1.0.57 in e:\\program files (x86)\\conda\\lib\\site-packages (1.0.57)\n",
      "Requirement already satisfied: scipy in e:\\program files (x86)\\conda\\lib\\site-packages (from fastai==1.0.57) (1.9.1)\n",
      "Requirement already satisfied: bottleneck in e:\\program files (x86)\\conda\\lib\\site-packages (from fastai==1.0.57) (1.3.5)\n",
      "Requirement already satisfied: matplotlib in e:\\program files (x86)\\conda\\lib\\site-packages (from fastai==1.0.57) (3.5.2)\n",
      "Requirement already satisfied: requests in e:\\program files (x86)\\conda\\lib\\site-packages (from fastai==1.0.57) (2.28.1)\n",
      "Requirement already satisfied: beautifulsoup4 in e:\\program files (x86)\\conda\\lib\\site-packages (from fastai==1.0.57) (4.11.1)\n",
      "Requirement already satisfied: packaging in e:\\program files (x86)\\conda\\lib\\site-packages (from fastai==1.0.57) (21.3)\n",
      "Requirement already satisfied: numexpr in e:\\program files (x86)\\conda\\lib\\site-packages (from fastai==1.0.57) (2.8.3)\n",
      "Requirement already satisfied: fastprogress>=0.1.19 in e:\\program files (x86)\\conda\\lib\\site-packages (from fastai==1.0.57) (1.0.3)\n",
      "Requirement already satisfied: numpy>=1.15 in e:\\program files (x86)\\conda\\lib\\site-packages (from fastai==1.0.57) (1.21.5)\n",
      "Requirement already satisfied: spacy>=2.0.18 in e:\\program files (x86)\\conda\\lib\\site-packages (from fastai==1.0.57) (3.4.4)\n",
      "Requirement already satisfied: nvidia-ml-py3 in e:\\program files (x86)\\conda\\lib\\site-packages (from fastai==1.0.57) (7.352.0)\n",
      "Requirement already satisfied: Pillow in e:\\program files (x86)\\conda\\lib\\site-packages (from fastai==1.0.57) (9.2.0)\n",
      "Requirement already satisfied: pyyaml in e:\\program files (x86)\\conda\\lib\\site-packages (from fastai==1.0.57) (6.0)\n",
      "Requirement already satisfied: torchvision in e:\\program files (x86)\\conda\\lib\\site-packages (from fastai==1.0.57) (0.14.1)\n",
      "Requirement already satisfied: torch>=1.0.0 in e:\\program files (x86)\\conda\\lib\\site-packages (from fastai==1.0.57) (1.13.1)\n",
      "Requirement already satisfied: pandas in e:\\program files (x86)\\conda\\lib\\site-packages (from fastai==1.0.57) (1.4.4)\n",
      "Requirement already satisfied: pathy>=0.3.5 in e:\\program files (x86)\\conda\\lib\\site-packages (from spacy>=2.0.18->fastai==1.0.57) (0.10.1)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in e:\\program files (x86)\\conda\\lib\\site-packages (from spacy>=2.0.18->fastai==1.0.57) (0.7.0)\n",
      "Requirement already satisfied: setuptools in e:\\program files (x86)\\conda\\lib\\site-packages (from spacy>=2.0.18->fastai==1.0.57) (63.4.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in e:\\program files (x86)\\conda\\lib\\site-packages (from spacy>=2.0.18->fastai==1.0.57) (2.4.5)\n",
      "Requirement already satisfied: jinja2 in e:\\program files (x86)\\conda\\lib\\site-packages (from spacy>=2.0.18->fastai==1.0.57) (2.11.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in e:\\program files (x86)\\conda\\lib\\site-packages (from spacy>=2.0.18->fastai==1.0.57) (1.10.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in e:\\program files (x86)\\conda\\lib\\site-packages (from spacy>=2.0.18->fastai==1.0.57) (5.2.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in e:\\program files (x86)\\conda\\lib\\site-packages (from spacy>=2.0.18->fastai==1.0.57) (2.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in e:\\program files (x86)\\conda\\lib\\site-packages (from spacy>=2.0.18->fastai==1.0.57) (4.64.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in e:\\program files (x86)\\conda\\lib\\site-packages (from spacy>=2.0.18->fastai==1.0.57) (1.0.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in e:\\program files (x86)\\conda\\lib\\site-packages (from spacy>=2.0.18->fastai==1.0.57) (3.3.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in e:\\program files (x86)\\conda\\lib\\site-packages (from spacy>=2.0.18->fastai==1.0.57) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in e:\\program files (x86)\\conda\\lib\\site-packages (from spacy>=2.0.18->fastai==1.0.57) (3.0.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in e:\\program files (x86)\\conda\\lib\\site-packages (from spacy>=2.0.18->fastai==1.0.57) (3.0.11)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in e:\\program files (x86)\\conda\\lib\\site-packages (from spacy>=2.0.18->fastai==1.0.57) (0.10.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in e:\\program files (x86)\\conda\\lib\\site-packages (from spacy>=2.0.18->fastai==1.0.57) (1.0.9)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in e:\\program files (x86)\\conda\\lib\\site-packages (from spacy>=2.0.18->fastai==1.0.57) (8.1.6)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in e:\\program files (x86)\\conda\\lib\\site-packages (from packaging->fastai==1.0.57) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\program files (x86)\\conda\\lib\\site-packages (from requests->fastai==1.0.57) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\program files (x86)\\conda\\lib\\site-packages (from requests->fastai==1.0.57) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in e:\\program files (x86)\\conda\\lib\\site-packages (from requests->fastai==1.0.57) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in e:\\program files (x86)\\conda\\lib\\site-packages (from requests->fastai==1.0.57) (2.0.4)\n",
      "Requirement already satisfied: typing_extensions in e:\\program files (x86)\\conda\\lib\\site-packages (from torch>=1.0.0->fastai==1.0.57) (4.3.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in e:\\program files (x86)\\conda\\lib\\site-packages (from beautifulsoup4->fastai==1.0.57) (2.3.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in e:\\program files (x86)\\conda\\lib\\site-packages (from matplotlib->fastai==1.0.57) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in e:\\program files (x86)\\conda\\lib\\site-packages (from matplotlib->fastai==1.0.57) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in e:\\program files (x86)\\conda\\lib\\site-packages (from matplotlib->fastai==1.0.57) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in e:\\program files (x86)\\conda\\lib\\site-packages (from matplotlib->fastai==1.0.57) (4.25.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\program files (x86)\\conda\\lib\\site-packages (from pandas->fastai==1.0.57) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in e:\\program files (x86)\\conda\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->fastai==1.0.57) (1.16.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in e:\\program files (x86)\\conda\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy>=2.0.18->fastai==1.0.57) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in e:\\program files (x86)\\conda\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy>=2.0.18->fastai==1.0.57) (0.0.3)\n",
      "Requirement already satisfied: colorama in e:\\program files (x86)\\conda\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy>=2.0.18->fastai==1.0.57) (0.4.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in e:\\program files (x86)\\conda\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy>=2.0.18->fastai==1.0.57) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in e:\\program files (x86)\\conda\\lib\\site-packages (from jinja2->spacy>=2.0.18->fastai==1.0.57) (2.0.1)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oeArf6jVVMzl",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "#@title Import dependencies {display-mode: \"form\"}\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from enum import Enum\n",
    "\n",
    "from fastai import *\n",
    "from fastai.text import *\n",
    "from fastai.callbacks import *\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "from pytorch_transformers import BertTokenizer, BertPreTrainedModel, BertModel, BertConfig\n",
    "from pytorch_transformers import AdamW"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CallbackHandler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_22692\\1635576006.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mfastai\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 10\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mfastai\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtext\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     11\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mfastai\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcallbacks\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel_selection\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtrain_test_split\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mStratifiedKFold\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Program Files (x86)\\Conda\\lib\\site-packages\\fastai\\text\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[1;33m.\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mbasics\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbasics\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mlearner\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mtransform\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Program Files (x86)\\Conda\\lib\\site-packages\\fastai\\basics.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mbasic_train\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mcallback\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mcore\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mbasic_data\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mdata_block\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Program Files (x86)\\Conda\\lib\\site-packages\\fastai\\basic_train.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     20\u001B[0m def loss_batch(model:nn.Module, xb:Tensor, yb:Tensor, loss_func:OptLossFunc=None, opt:OptOptimizer=None,\n\u001B[1;32m---> 21\u001B[1;33m                cb_handler:Optional[CallbackHandler]=None)->Tuple[Union[Tensor,int,float,str]]:\n\u001B[0m\u001B[0;32m     22\u001B[0m     \u001B[1;34m\"Calculate loss and metrics for a batch, call out to callbacks as necessary.\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     23\u001B[0m     \u001B[0mcb_handler\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mifnone\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcb_handler\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mCallbackHandler\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'CallbackHandler' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FgN8QQ8YYkpU",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "#@title Define Config classes {display-mode: \"form\"}\n",
    "class Config(dict):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "    \n",
    "    def set(self, key, val):\n",
    "        self[key] = val\n",
    "        setattr(self, key, val)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tIyeBgjnYp7M",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# adapt this to your needs!\n",
    "config_data = Config(\n",
    "    root_folder = '.', # where is the root folder? Keep it that way if you want to load from Google Drive\n",
    "    model_path = '/models/', # where is the folder for the model(s); relative to the root\n",
    "    model_name = 'NoRBERT_Task1_NFR_e10_NoSampling.pkl', # what is the model name? \n",
    ")\n",
    "\n",
    "load_from_gdrive = True # True, if you want to use Google Drive; else, False\n",
    "gdrive_root_folder = '/content/drive/My Drive/Code/Apply_Pretrained_Model/' # Set this to the Google Drive path. Starts with '/content/drive/' and then usually 'My Drive/*' for the files in your Drive"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "Wtzha3q7QjjU",
    "colab": {}
   },
   "source": [
    "#@title Check, if and what kind of GPU is used {display-mode: \"form\"}\n",
    "cuda_available = torch.cuda.is_available()\n",
    "if cuda_available:\n",
    "    curr_device = torch.cuda.current_device()\n",
    "    print(torch.cuda.get_device_name(curr_device))\n",
    "device = torch.device(\"cuda\" if cuda_available else \"cpu\")\n",
    "device"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LysOxgPvW0am",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "#@title Init loading from Google Drive, if set in config above {display-mode: \"form\"}\n",
    "if load_from_gdrive:\n",
    "    from google.colab import drive\n",
    "    # Connect to drive to load the corpus from there\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    config_data.root_folder = gdrive_root_folder"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "TNRRj6jIJrp2",
    "colab": {}
   },
   "source": [
    "#@title Load/Define NoRBERT classes {display-mode: \"form\"}\n",
    "class FastAiBertTokenizer(BaseTokenizer):\n",
    "    \"\"\"Wrapper around BertTokenizer to be compatible with fast.ai\"\"\"\n",
    "    def __init__(self, tokenizer: BertTokenizer, max_seq_len: int=512, **kwargs):\n",
    "        self._pretrained_tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self\n",
    "\n",
    "    def tokenizer(self, t:str):\n",
    "        \"\"\"Limits the maximum sequence length. Prepend with [CLS] and append [SEP]\"\"\"\n",
    "        return [\"[CLS]\"] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [\"[SEP]\"]\n",
    "\n",
    "## \n",
    "\n",
    "class BertTokenizeProcessor(TokenizeProcessor):\n",
    "    \"\"\"Special Tokenizer, where we remove sos/eos tokens since we add that ourselves in the tokenizer.\"\"\"\n",
    "    def __init__(self, tokenizer):\n",
    "        super().__init__(tokenizer=tokenizer, include_bos=False, include_eos=False)\n",
    "\n",
    "class BertNumericalizeProcessor(NumericalizeProcessor):\n",
    "    \"\"\"Use a custom vocabulary to match the original BERT model.\"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, vocab=Vocab(list(bert_tok.vocab.keys())), **kwargs)\n",
    "\n",
    "def get_bert_processor(tokenizer:Tokenizer=None, vocab:Vocab=None):\n",
    "    return [BertTokenizeProcessor(tokenizer=tokenizer),\n",
    "            NumericalizeProcessor(vocab=vocab)]\n",
    "\n",
    "class BertDataBunch(TextDataBunch):\n",
    "    @classmethod\n",
    "    def from_df(cls, path:PathOrStr, train_df:DataFrame, valid_df:DataFrame, test_df:Optional[DataFrame]=None,\n",
    "              tokenizer:Tokenizer=None, vocab:Vocab=None, classes:Collection[str]=None, text_cols:IntsOrStrs=1,\n",
    "              label_cols:IntsOrStrs=0, **kwargs) -> DataBunch:\n",
    "        \"Create a `TextDataBunch` from DataFrames.\"\n",
    "        p_kwargs, kwargs = split_kwargs_by_func(kwargs, get_bert_processor)\n",
    "        # use our custom processors while taking tokenizer and vocab as kwargs\n",
    "        processor = get_bert_processor(tokenizer=tokenizer, vocab=vocab, **p_kwargs)\n",
    "        if classes is None and is_listy(label_cols) and len(label_cols) > 1: classes = label_cols\n",
    "        src = ItemLists(path, TextList.from_df(train_df, path, cols=text_cols, processor=processor),\n",
    "                      TextList.from_df(valid_df, path, cols=text_cols, processor=processor))\n",
    "        src = src.label_for_lm() if cls==TextLMDataBunch else src.label_from_df(cols=label_cols, classes=classes)\n",
    "        if test_df is not None: src.add_test(TextList.from_df(test_df, path, cols=text_cols))\n",
    "        return src.databunch(**kwargs)\n",
    "\n",
    "##\n",
    "\n",
    "class BertTextClassifier(BertPreTrainedModel):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        config = BertConfig.from_pretrained(model_name)\n",
    "        super(BertTextClassifier, self).__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        self.bert = BertModel.from_pretrained(model_name, config=config)\n",
    "        \n",
    "        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(self.config.hidden_size, num_labels)\n",
    "\n",
    "        #self.apply(self.init_weights)\n",
    "    \n",
    "    def forward(self, tokens, labels=None, position_ids=None, token_type_ids=None, attention_mask=None, head_mask=None):\n",
    "        outputs = self.bert(tokens, position_ids=position_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, head_mask=head_mask)\n",
    "        \n",
    "        pooled_output = outputs[1]\n",
    "        # According to documentation of pytorch-transformers, pooled output might not be the best \n",
    "        # and youâ€™re often better with averaging or pooling the sequence of hidden-states for the whole input sequence \n",
    "        #hidden_states = outputs[0]\n",
    "        #pooled_output = torch.mean(hidden_states, 1)\n",
    "\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(dropout_output)\n",
    "\n",
    "        activation = nn.Softmax(dim=1)\n",
    "        probs = activation(logits)   \n",
    "\n",
    "        return logits"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OK17yQAGV_bM",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "#@title Load classifier {display-mode: \"form\"}\n",
    "classifier = load_learner(config_data.root_folder + config_data.model_path, config_data.model_name)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9IxRhtyyYbht",
    "colab_type": "text"
   },
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CxsXG_HaXg1s",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "#@title {display-mode: \"form\"}\n",
    "def predict(classifier, text):\n",
    "    prediction = classifier.predict(text)\n",
    "    prediction_class = prediction[1]\n",
    "    label = classifier.data.classes[prediction_class]\n",
    "    return label"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UMMYic_pa5Sy",
    "colab_type": "code",
    "cellView": "both",
    "colab": {}
   },
   "source": [
    "#Set the labels, the classifier will output (in correct order!)\n",
    "labels = ['F', 'NFR']"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HISh0sGGeVbj",
    "colab_type": "code",
    "cellView": "form",
    "colab": {}
   },
   "source": [
    "#@title Set input requirement\n",
    "input_requirement =  'The system shall display Events in a vertical table by time.'  #@param {type: \"string\"}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ztKNDBCVaGfK",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "#@title Predict output label {display-mode: \"form\"}\n",
    "pred_class = predict(classifier, input_requirement)\n",
    "print(labels[pred_class])"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
