{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Task1_F_NFR_classification.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Binary Classification of functional and non-functional requirements on Promise NFR Dataset"
   ],
   "metadata": {
    "id": "Vg6nFRifQ1AV"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This notebook includes all code needed to train and evaluate a binary classifier for predicting whether a requirement in the Promise NFR dataset is a functional or non-functional requirement."
   ],
   "metadata": {
    "id": "CSO9nA7LmQdh"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note: some cells are hidden and only the title is shown. To display the code, double-click the cell to switch the display mode."
   ],
   "metadata": {
    "id": "tnp6fvcFmpQN"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare\n",
    "Install required libraries and import packages"
   ],
   "metadata": {
    "id": "-djxLyuc-Opk"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#@title Install needed libraries {display-mode: \"form\"}\n",
    "!pip install pytorch-transformers fastprogress\n",
    "!pip install fastai==1.0.57"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-transformers in e:\\program files (x86)\\conda\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: fastprogress in e:\\program files (x86)\\conda\\lib\\site-packages (1.0.3)\n",
      "Requirement already satisfied: boto3 in e:\\program files (x86)\\conda\\lib\\site-packages (from pytorch-transformers) (1.24.28)\n",
      "Requirement already satisfied: regex in e:\\program files (x86)\\conda\\lib\\site-packages (from pytorch-transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in e:\\program files (x86)\\conda\\lib\\site-packages (from pytorch-transformers) (2.28.1)\n",
      "Requirement already satisfied: sacremoses in e:\\program files (x86)\\conda\\lib\\site-packages (from pytorch-transformers) (0.0.53)\n",
      "Requirement already satisfied: torch>=1.0.0 in e:\\program files (x86)\\conda\\lib\\site-packages (from pytorch-transformers) (1.13.1)\n",
      "Requirement already satisfied: tqdm in e:\\program files (x86)\\conda\\lib\\site-packages (from pytorch-transformers) (4.64.1)\n",
      "Requirement already satisfied: sentencepiece in e:\\program files (x86)\\conda\\lib\\site-packages (from pytorch-transformers) (0.1.97)\n",
      "Requirement already satisfied: numpy in e:\\program files (x86)\\conda\\lib\\site-packages (from pytorch-transformers) (1.21.5)\n",
      "Requirement already satisfied: typing_extensions in e:\\program files (x86)\\conda\\lib\\site-packages (from torch>=1.0.0->pytorch-transformers) (4.3.0)\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.28 in e:\\program files (x86)\\conda\\lib\\site-packages (from boto3->pytorch-transformers) (1.27.28)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in e:\\program files (x86)\\conda\\lib\\site-packages (from boto3->pytorch-transformers) (0.6.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in e:\\program files (x86)\\conda\\lib\\site-packages (from boto3->pytorch-transformers) (0.10.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in e:\\program files (x86)\\conda\\lib\\site-packages (from requests->pytorch-transformers) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\program files (x86)\\conda\\lib\\site-packages (from requests->pytorch-transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in e:\\program files (x86)\\conda\\lib\\site-packages (from requests->pytorch-transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\program files (x86)\\conda\\lib\\site-packages (from requests->pytorch-transformers) (2022.9.14)\n",
      "Requirement already satisfied: click in e:\\program files (x86)\\conda\\lib\\site-packages (from sacremoses->pytorch-transformers) (8.0.4)\n",
      "Requirement already satisfied: joblib in e:\\program files (x86)\\conda\\lib\\site-packages (from sacremoses->pytorch-transformers) (1.1.0)\n",
      "Requirement already satisfied: six in e:\\program files (x86)\\conda\\lib\\site-packages (from sacremoses->pytorch-transformers) (1.16.0)\n",
      "Requirement already satisfied: colorama in e:\\program files (x86)\\conda\\lib\\site-packages (from tqdm->pytorch-transformers) (0.4.5)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in e:\\program files (x86)\\conda\\lib\\site-packages (from botocore<1.28.0,>=1.27.28->boto3->pytorch-transformers) (2.8.2)\n",
      "Collecting fastai==1.0.57\n",
      "  Downloading fastai-1.0.57-py3-none-any.whl (233 kB)\n",
      "     ------------------------------------ 233.3/233.3 kB 375.6 kB/s eta 0:00:00\n",
      "Requirement already satisfied: fastprogress>=0.1.19 in e:\\program files (x86)\\conda\\lib\\site-packages (from fastai==1.0.57) (1.0.3)\n",
      "Collecting nvidia-ml-py3\n",
      "  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: scipy in e:\\program files (x86)\\conda\\lib\\site-packages (from fastai==1.0.57) (1.9.1)\n",
      "Requirement already satisfied: numpy>=1.15 in e:\\program files (x86)\\conda\\lib\\site-packages (from fastai==1.0.57) (1.21.5)\n",
      "Requirement already satisfied: Pillow in e:\\program files (x86)\\conda\\lib\\site-packages (from fastai==1.0.57) (9.2.0)\n",
      "Requirement already satisfied: torch>=1.0.0 in e:\\program files (x86)\\conda\\lib\\site-packages (from fastai==1.0.57) (1.13.1)\n",
      "Requirement already satisfied: numexpr in e:\\program files (x86)\\conda\\lib\\site-packages (from fastai==1.0.57) (2.8.3)\n",
      "Requirement already satisfied: requests in e:\\program files (x86)\\conda\\lib\\site-packages (from fastai==1.0.57) (2.28.1)\n",
      "Requirement already satisfied: pandas in e:\\program files (x86)\\conda\\lib\\site-packages (from fastai==1.0.57) (1.4.4)\n",
      "Requirement already satisfied: packaging in e:\\program files (x86)\\conda\\lib\\site-packages (from fastai==1.0.57) (21.3)\n",
      "Requirement already satisfied: beautifulsoup4 in e:\\program files (x86)\\conda\\lib\\site-packages (from fastai==1.0.57) (4.11.1)\n",
      "Requirement already satisfied: torchvision in e:\\program files (x86)\\conda\\lib\\site-packages (from fastai==1.0.57) (0.14.1)\n",
      "Requirement already satisfied: pyyaml in e:\\program files (x86)\\conda\\lib\\site-packages (from fastai==1.0.57) (6.0)\n",
      "Collecting spacy>=2.0.18\n",
      "  Downloading spacy-3.4.4-cp39-cp39-win_amd64.whl (11.9 MB)\n",
      "     ---------------------------------------- 11.9/11.9 MB 9.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: matplotlib in e:\\program files (x86)\\conda\\lib\\site-packages (from fastai==1.0.57) (3.5.2)\n",
      "Requirement already satisfied: bottleneck in e:\\program files (x86)\\conda\\lib\\site-packages (from fastai==1.0.57) (1.3.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in e:\\program files (x86)\\conda\\lib\\site-packages (from spacy>=2.0.18->fastai==1.0.57) (4.64.1)\n",
      "Collecting wasabi<1.1.0,>=0.9.1\n",
      "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: setuptools in e:\\program files (x86)\\conda\\lib\\site-packages (from spacy>=2.0.18->fastai==1.0.57) (63.4.1)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting thinc<8.2.0,>=8.1.0\n",
      "  Downloading thinc-8.1.6-cp39-cp39-win_amd64.whl (1.3 MB)\n",
      "     ---------------------------------------- 1.3/1.3 MB 13.5 MB/s eta 0:00:00\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp39-cp39-win_amd64.whl (18 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.5-cp39-cp39-win_amd64.whl (481 kB)\n",
      "     ------------------------------------- 481.4/481.4 kB 15.2 MB/s eta 0:00:00\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp39-cp39-win_amd64.whl (96 kB)\n",
      "     ---------------------------------------- 96.8/96.8 kB ? eta 0:00:00\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in e:\\program files (x86)\\conda\\lib\\site-packages (from spacy>=2.0.18->fastai==1.0.57) (5.2.1)\n",
      "Requirement already satisfied: jinja2 in e:\\program files (x86)\\conda\\lib\\site-packages (from spacy>=2.0.18->fastai==1.0.57) (2.11.3)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.10\n",
      "  Downloading spacy_legacy-3.0.11-py2.py3-none-any.whl (24 kB)\n",
      "Collecting typer<0.8.0,>=0.3.0\n",
      "  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp39-cp39-win_amd64.whl (30 kB)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.10.1-py3-none-any.whl (48 kB)\n",
      "     ---------------------------------------- 48.9/48.9 kB 2.4 MB/s eta 0:00:00\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "     ---------------------------------------- 181.6/181.6 kB ? eta 0:00:00\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
      "  Downloading pydantic-1.10.4-cp39-cp39-win_amd64.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 17.0 MB/s eta 0:00:00\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in e:\\program files (x86)\\conda\\lib\\site-packages (from packaging->fastai==1.0.57) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in e:\\program files (x86)\\conda\\lib\\site-packages (from requests->fastai==1.0.57) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\program files (x86)\\conda\\lib\\site-packages (from requests->fastai==1.0.57) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\program files (x86)\\conda\\lib\\site-packages (from requests->fastai==1.0.57) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in e:\\program files (x86)\\conda\\lib\\site-packages (from requests->fastai==1.0.57) (1.26.11)\n",
      "Requirement already satisfied: typing_extensions in e:\\program files (x86)\\conda\\lib\\site-packages (from torch>=1.0.0->fastai==1.0.57) (4.3.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in e:\\program files (x86)\\conda\\lib\\site-packages (from beautifulsoup4->fastai==1.0.57) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in e:\\program files (x86)\\conda\\lib\\site-packages (from matplotlib->fastai==1.0.57) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in e:\\program files (x86)\\conda\\lib\\site-packages (from matplotlib->fastai==1.0.57) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in e:\\program files (x86)\\conda\\lib\\site-packages (from matplotlib->fastai==1.0.57) (1.4.2)\n",
      "Requirement already satisfied: cycler>=0.10 in e:\\program files (x86)\\conda\\lib\\site-packages (from matplotlib->fastai==1.0.57) (0.11.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\program files (x86)\\conda\\lib\\site-packages (from pandas->fastai==1.0.57) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in e:\\program files (x86)\\conda\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->fastai==1.0.57) (1.16.0)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.0.3-py3-none-any.whl (32 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.9-cp39-cp39-win_amd64.whl (7.0 MB)\n",
      "     ---------------------------------------- 7.0/7.0 MB 25.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in e:\\program files (x86)\\conda\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy>=2.0.18->fastai==1.0.57) (0.4.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in e:\\program files (x86)\\conda\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy>=2.0.18->fastai==1.0.57) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in e:\\program files (x86)\\conda\\lib\\site-packages (from jinja2->spacy>=2.0.18->fastai==1.0.57) (2.0.1)\n",
      "Building wheels for collected packages: nvidia-ml-py3\n",
      "  Building wheel for nvidia-ml-py3 (setup.py): started\n",
      "  Building wheel for nvidia-ml-py3 (setup.py): finished with status 'done'\n",
      "  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19172 sha256=ecb7ee8cddc0ec194d9e550c8823647292259a28e512cfc6ea29dc392b53637c\n",
      "  Stored in directory: c:\\users\\hanlu\\appdata\\local\\pip\\cache\\wheels\\f6\\d8\\b0\\15cfd7805d39250ac29318105f09b1750683387630d68423e1\n",
      "Successfully built nvidia-ml-py3\n",
      "Installing collected packages: wasabi, nvidia-ml-py3, cymem, spacy-loggers, spacy-legacy, pydantic, murmurhash, langcodes, catalogue, blis, typer, srsly, preshed, pathy, confection, thinc, spacy, fastai\n",
      "Successfully installed blis-0.7.9 catalogue-2.0.8 confection-0.0.3 cymem-2.0.7 fastai-1.0.57 langcodes-3.3.0 murmurhash-1.0.9 nvidia-ml-py3-7.352.0 pathy-0.10.1 preshed-3.0.8 pydantic-1.10.4 spacy-3.4.4 spacy-legacy-3.0.11 spacy-loggers-1.0.4 srsly-2.4.5 thinc-8.1.6 typer-0.7.0 wasabi-0.10.1\n"
     ]
    }
   ],
   "metadata": {
    "id": "Epk5taxa99eI",
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#@title Import python packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from enum import Enum\n",
    "from fastai import *\n",
    "from fastai.text import *\n",
    "from fastai.callback import *\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "from pytorch_transformers import BertTokenizer, BertPreTrainedModel, BertModel, BertConfig\n",
    "from pytorch_transformers import AdamW\n",
    "\n",
    "from fastprogress import master_bar, progress_bar\n",
    "from datetime import datetime"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CallbackHandler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_26428\\339765355.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0menum\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mEnum\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mfastai\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 11\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mfastai\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtext\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     12\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mfastai\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcallback\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel_selection\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtrain_test_split\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mStratifiedKFold\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Program Files (x86)\\Conda\\lib\\site-packages\\fastai\\text\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[1;33m.\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mbasics\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbasics\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mlearner\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mtransform\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Program Files (x86)\\Conda\\lib\\site-packages\\fastai\\basics.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mbasic_train\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mcallback\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mcore\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mbasic_data\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mdata_block\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Program Files (x86)\\Conda\\lib\\site-packages\\fastai\\basic_train.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     20\u001B[0m def loss_batch(model:nn.Module, xb:Tensor, yb:Tensor, loss_func:OptLossFunc=None, opt:OptOptimizer=None,\n\u001B[1;32m---> 21\u001B[1;33m                cb_handler:Optional[CallbackHandler]=None)->Tuple[Union[Tensor,int,float,str]]:\n\u001B[0m\u001B[0;32m     22\u001B[0m     \u001B[1;34m\"Calculate loss and metrics for a batch, call out to callbacks as necessary.\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     23\u001B[0m     \u001B[0mcb_handler\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mifnone\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcb_handler\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mCallbackHandler\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'CallbackHandler' is not defined"
     ]
    }
   ],
   "metadata": {
    "id": "Fr6bTWdl-XzF",
    "cellView": "form"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#@title Check, if and what kind of GPU is used\n",
    "def get_memory_usage():\n",
    "    return torch.cuda.memory_allocated(device)/1000000\n",
    "\n",
    "def get_memory_usage_str():\n",
    "    return 'Memory usage: {:.2f} MB'.format(get_memory_usage())\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "if cuda_available:\n",
    "    curr_device = torch.cuda.current_device()\n",
    "    print(torch.cuda.get_device_name(curr_device))\n",
    "device = torch.device(\"cuda\" if cuda_available else \"cpu\")\n",
    "device"
   ],
   "outputs": [],
   "metadata": {
    "id": "Wtzha3q7QjjU",
    "cellView": "form"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define configuration used in this experiment run"
   ],
   "metadata": {
    "id": "WJRAlPs5rRIy"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create config and set hyperparameters.\n",
    "One can configure:\n",
    "\n",
    "\n",
    "*   BERT model to use (model_name)\n",
    "*   Learning Rate to use (max_lr)\n",
    "*   Momentum (moms)\n",
    "*   Epoch number for training (epochs)\n",
    "*   Badge size for training (bs)\n",
    "*   Weight decay for training (weight_decay)\n",
    "*   Maximal sequence length (max_seq_len)\n",
    "*   Train size used for both test/train and train/validation split (train_size)\n",
    "*   Loss function used for training (loss_func)\n",
    "*   The random seed used for shuffling, sampling and splitting (seed)\n",
    "*   Whether, or not to use early stopping (es)\n",
    "*   The minimal delta used to indicate early stopping (min_delta)\n",
    "*   The number of epochs that need to undergo this delta to early stop training (patience)\n",
    "*   The way of folding used for this experiment (either test/train split (No), ten-fold cross validation (TenFold), or project specific folding (ProjFold))\n",
    "*   Which kind of sampling to use (either OverSampling minority class, UnderSampling majority class, or NoSampling at all)\n",
    "\n",
    "*   Which class to predict (clazz)\n",
    "\n",
    "Further one can configure, where to get the dataset from and where to save log, result and model files.\n",
    "Two booleans are provided to decide whether to:\n",
    "1. load data from Google Drive or download data from zenodo and to\n",
    "2. save the model file.\n",
    "\n"
   ],
   "metadata": {
    "id": "mOKXVgJgGtYV"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Config(dict):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "    \n",
    "    def set(self, key, val):\n",
    "        self[key] = val\n",
    "        setattr(self, key, val)\n",
    "\n",
    "class Fold(Enum):\n",
    "  No = 1\n",
    "  TenFold = 2\n",
    "  ProjFold = 3\n",
    "\n",
    "class Sampling(Enum):\n",
    "  NoSampling = 1\n",
    "  UnderSampling = 2\n",
    "  OverSampling = 3\n",
    "\n",
    "config = Config(\n",
    "    num_labels = 2, # will be set automatically afterwards\n",
    "    model_name=\"bert-base-cased\", # bert_base_uncased, bert_large_cased, bert_large_uncased\n",
    "    max_lr=2e-5, # default: 2e-5\n",
    "    moms=(0.8, 0.7), # default: (0.8, 0.7); alt.(0.95, 0.85)\n",
    "    epochs=16, # 10, 16, 32, 50\n",
    "    bs=16, # default: 16\n",
    "    weight_decay = 0.01,\n",
    "    max_seq_len=128, # 50, 128\n",
    "    train_size=0.75, # 0.8\n",
    "    loss_func=nn.CrossEntropyLoss(),\n",
    "    seed=904727489, #default: 904727489, 42 (as in Dalpiaz) or None\n",
    "    es = False, # True\n",
    "    min_delta = 0.01,\n",
    "    patience = 3,\n",
    "    fold = Fold.No, # Fold.No, Fold.TenFold, Fold.ProjFold\n",
    "    sampling = Sampling.NoSampling, #Sampling.UnderSampling, Sampling.NoSampling, Sampling.OverSampling\n",
    ")\n",
    "\n",
    "clazz = 'NFR' # class to train classification on\n",
    "\n",
    "config_data = Config(\n",
    "    root_folder = '.', # where is the root folder? Keep it that way if you want to load from Google Drive\n",
    "    data_folder = '/', # where is the folder containing the datasets; relative to root\n",
    "    train_data = ['promise_nfr.csv'], # dataset file to use\n",
    "    label_column = clazz,\n",
    "    log_folder_name = '/log/',\n",
    "    log_file = clazz + '_' + Fold(config.fold).name + '_' + Sampling(config.sampling).name + '_classifierPredictions_' + datetime.now().strftime('%Y%m%d-%H%M') + '.txt', # log-file name (make sure log folder exists)\n",
    "    result_file = clazz + '_' + Fold(config.fold).name + '_' + Sampling(config.sampling).name + '_classifierResults_' + datetime.now().strftime('%Y%m%d-%H%M') + '.txt', # result-file name (make sure log folder exists)\n",
    "    model_path = '/models/', # where is the folder for the model(s); relative to the root\n",
    "    model_name = 'NoRBERT.pkl', # what is the model name? \n",
    "    gdrive_root_folder = '/content/drive/My Drive/Code/Task1_to_3_original_Promise_NFR_dataset/', # Set this to the Google Drive path. Starts with '/content/drive/' and then usually 'My Drive/*' for the files in your Drive\n",
    "\n",
    "    orig_data_set_zip = 'https://zenodo.org/record/5541679/files/NoRBERT_RE20_Paper65.zip', # link to the data set (on zenodo). DO NOT CHANGE!\n",
    "    orig_data_zip_name = 'NoRBERT_RE20_Paper65.zip', # DO NOT CHANGE\n",
    "    orig_data_file_in_zip = 'Code/Task1_to_3_original_Promise_NFR_dataset/promise_nfr.csv', # DO NOT CHANGE\n",
    "    \n",
    "    # Project split to use, either p-fold (as in Dalpiaz) or loPo\n",
    "    #project_fold = [[3, 9, 11], [1, 5, 12], [6, 10, 13], [1, 8, 14], [3, 12, 15], [2, 5, 11], [6, 9, 14], [7, 8, 13], [2, 4, 15], [4, 7, 10] ], # p-fold\n",
    "    project_fold = [[1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15] ], # loPo\n",
    ")\n",
    "\n",
    "load_from_gdrive = False # True, if you want to use Google Drive; else, False\n",
    "save_model = True # True, if you want to use save the model file (make sure the \"models\" folder exists)"
   ],
   "outputs": [],
   "metadata": {
    "id": "i0lgLyC6Gsnf"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To import the dataset, first we have to either load the data set from zenodo (and unzip the needed file) or connect to our Google drive (if data should be loaded from gdrive). To connect to our Google drive, we have to authenticate the access and mount the drive."
   ],
   "metadata": {
    "id": "SVU_viFX-ezy"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#@title Prepare data loading: Init loading from Google Drive, if set in config above. Else, download the data set from zenodo (using wget) {display-mode: \"form\"}\n",
    "if load_from_gdrive:\n",
    "    from google.colab import drive\n",
    "    # Connect to drive to load the corpus from there\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    config_data.root_folder = config_data.gdrive_root_folder\n",
    "else:\n",
    "    # If the file does not exist already, download the zip and extract the needed file\n",
    "    data_path = config_data.root_folder + config_data.data_folder + config_data.train_data[0]\n",
    "    data_file = Path(data_path)\n",
    "    if not data_file.exists():\n",
    "        !wget {config_data.orig_data_set_zip}\n",
    "        import zipfile\n",
    "        with zipfile.ZipFile(config_data.orig_data_zip_name) as z:\n",
    "            with open(data_path, 'wb') as f:\n",
    "                f.write(z.read(config_data.orig_data_file_in_zip))\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "OmGISBrhW-VJ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#@title Define logging functions and seed generation {display-mode: \"form\"}\n",
    "def initLog():\n",
    "    logfolder = config_data.root_folder + config_data.log_folder_name\n",
    "   \n",
    "    if not os.path.isdir(logfolder):\n",
    "      print(\"Log folder does not exist, trying to create folder.\")\n",
    "      try:\n",
    "        os.mkdir(logfolder)\n",
    "      except OSError:\n",
    "        print (\"Creation of the directory %s failed\" % logfolder)\n",
    "      else:\n",
    "        print (\"Successfully created the directory %s\" % logfolder)\n",
    "    logfile = logfolder + config_data.log_file\n",
    "    log_txt = datetime.now().strftime('%Y-%m-%d %H:%M') + ' ' + get_info()\n",
    "    with open(logfile, 'w') as log:\n",
    "        log.write(log_txt + '\\n')\n",
    "\n",
    "def logLine(line):\n",
    "    logfile = config_data.root_folder + config_data.log_folder_name  + config_data.log_file\n",
    "    with open(logfile, 'a') as log:\n",
    "        log.write(line + '\\n')\n",
    "\n",
    "def logResult(result):\n",
    "    logfile = config_data.root_folder + config_data.log_folder_name + config_data.result_file\n",
    "    with open(logfile, 'a') as log:\n",
    "        log.write(get_info() + '\\n')\n",
    "        log.write(result + '\\n')\n",
    "\n",
    "def get_info():\n",
    "     model_config = 'model: {}, max_lr: {}, epochs: {}, bs: {}, train_size: {}, weight decay: {},  Seed: {}, Data: {}, Column: {}, EarlyStopping: {}:{};pat:{}'.format(config.model_name, config.max_lr, config.epochs, config.bs, config.train_size, config.weight_decay, config.seed, config_data.train_data, config_data.label_column, config.es, config.min_delta, config.patience)\n",
    "     return model_config\n",
    "\n",
    "def set_seed(seed):\n",
    "    if seed is None:\n",
    "        seed = random.randint(0, 2**31)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    return seed\n",
    "\n",
    "set_seed(config.seed)"
   ],
   "outputs": [],
   "metadata": {
    "id": "er1yzLHFQq1U"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Learner"
   ],
   "metadata": {
    "id": "1Tl06zRQjlKZ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#@title Create proper tokenizer for our data (adapting FastAiTokenizer to use BertTokenizer) {display-mode: \"form\"}\n",
    "class FastAiBertTokenizer(BaseTokenizer):\n",
    "    \"\"\"Wrapper around BertTokenizer to be compatible with fast.ai\"\"\"\n",
    "    def __init__(self, tokenizer: BertTokenizer, max_seq_len: int=512, **kwargs):\n",
    "        self._pretrained_tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self\n",
    "\n",
    "    def tokenizer(self, t:str):\n",
    "        \"\"\"Limits the maximum sequence length. Prepend with [CLS] and append [SEP]\"\"\"\n",
    "        return [\"[CLS]\"] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [\"[SEP]\"]\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "6anB63ppBAtB"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we can create our own databunch using the tokenizer above. Notice we're passing the include_bos=False and include_eos=False options. This is to prevent fastai from adding its own SOS/EOS tokens that will interfere with BERT's SOS/EOS tokens.\n",
    "\n",
    "We can pass our own list of Preprocessors to the databunch."
   ],
   "metadata": {
    "id": "1G8rFbEEJWyu"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#@title Define Processors and Databunch {display-mode: \"form\"}\n",
    "class BertTokenizeProcessor(TokenizeProcessor):\n",
    "    \"\"\"Special Tokenizer, where we remove sos/eos tokens since we add that ourselves in the tokenizer.\"\"\"\n",
    "    def __init__(self, tokenizer):\n",
    "        super().__init__(tokenizer=tokenizer, include_bos=False, include_eos=False)\n",
    "\n",
    "class BertNumericalizeProcessor(NumericalizeProcessor):\n",
    "    \"\"\"Use a custom vocabulary to match the original BERT model.\"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, vocab=Vocab(list(bert_tok.vocab.keys())), **kwargs)\n",
    "\n",
    "def get_bert_processor(tokenizer:Tokenizer=None, vocab:Vocab=None):\n",
    "    return [BertTokenizeProcessor(tokenizer=tokenizer),\n",
    "            NumericalizeProcessor(vocab=vocab)]\n",
    "\n",
    "class BertDataBunch(TextDataBunch):\n",
    "    @classmethod\n",
    "    def from_df(cls, path:PathOrStr, train_df:DataFrame, valid_df:DataFrame, test_df:Optional[DataFrame]=None,\n",
    "              tokenizer:Tokenizer=None, vocab:Vocab=None, classes:Collection[str]=None, text_cols:IntsOrStrs=1,\n",
    "              label_cols:IntsOrStrs=0, **kwargs) -> DataBunch:\n",
    "        \"Create a `TextDataBunch` from DataFrames.\"\n",
    "        p_kwargs, kwargs = split_kwargs_by_func(kwargs, get_bert_processor)\n",
    "        # use our custom processors while taking tokenizer and vocab as kwargs\n",
    "        processor = get_bert_processor(tokenizer=tokenizer, vocab=vocab, **p_kwargs)\n",
    "        if classes is None and is_listy(label_cols) and len(label_cols) > 1: classes = label_cols\n",
    "        src = ItemLists(path, TextList.from_df(train_df, path, cols=text_cols, processor=processor),\n",
    "                      TextList.from_df(valid_df, path, cols=text_cols, processor=processor))\n",
    "        src = src.label_for_lm() if cls==TextLMDataBunch else src.label_from_df(cols=label_cols, classes=classes)\n",
    "        if test_df is not None: src.add_test(TextList.from_df(test_df, path, cols=text_cols))\n",
    "        return src.databunch(**kwargs)"
   ],
   "outputs": [],
   "metadata": {
    "id": "TNRRj6jIJrp2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#@title Define own BertTextClassifier class{display-mode: \"form\"}\n",
    "class BertTextClassifier(BertPreTrainedModel):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        config = BertConfig.from_pretrained(model_name)\n",
    "        super(BertTextClassifier, self).__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        self.bert = BertModel.from_pretrained(model_name, config=config)\n",
    "        \n",
    "        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(self.config.hidden_size, num_labels)\n",
    "\n",
    "    \n",
    "    def forward(self, tokens, labels=None, position_ids=None, token_type_ids=None, attention_mask=None, head_mask=None):\n",
    "        outputs = self.bert(tokens, position_ids=position_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, head_mask=head_mask)\n",
    "        \n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(dropout_output)\n",
    "\n",
    "        activation = nn.Softmax(dim=1)\n",
    "        probs = activation(logits)   \n",
    "\n",
    "        return logits"
   ],
   "outputs": [],
   "metadata": {
    "id": "he_PRt9Q3eUB"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data\n"
   ],
   "metadata": {
    "id": "Ptp6NhIC_FQb"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the dataset"
   ],
   "metadata": {
    "id": "IwxQFKykzpQq"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#@title Define functions to load data {display-mode: \"form\"}\n",
    "def load_data(filename):\n",
    "    fpath = config_data.root_folder + config_data.data_folder + filename\n",
    "    print(fpath)\n",
    "    df = pd.read_csv(fpath, delimiter=';', header=0, encoding='utf8', names=['number', 'ProjectID', 'RequirementText', 'class', 'NFR', 'F', 'A', 'FT', 'L', 'LF', 'MN', 'O', 'PE', 'PO', 'SC', 'SE', 'US'])\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "def load_all_data(filenames):\n",
    "    df = load_data(filenames[0])\n",
    "    for i in range(1, len(filenames)):\n",
    "        df = df.append(load_data(filenames[i]))\n",
    "    return df\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "oeaTvNRTypP0"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#@title Actually load the dataset{display-mode: \"form\"}\n",
    "# load the train dataset\n",
    "df = load_all_data(config_data.train_data)\n",
    "input_col = 'RequirementText'\n",
    "# shuffle the dataset a bit and get the amount of classes\n",
    "df = df.sample(frac=1, axis=0, random_state = config.seed)\n",
    "config.num_labels = df[config_data.label_column].nunique()\n",
    "\n",
    "print(df.shape)\n",
    "print(df[config_data.label_column].value_counts())"
   ],
   "outputs": [],
   "metadata": {
    "id": "-6o6UU0tUYck"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#@title Create the dictionary that contains the labels along with their indices. This is useful for evaluation and similar. {display-mode: \"form\"}\n",
    "def create_label_indices(df):\n",
    "    #prepare label\n",
    "    labels = ['not_' + config_data.label_column, config_data.label_column]\n",
    "  \n",
    "    #create dict\n",
    "    labelDict = dict()\n",
    "    for i in range (0, len(labels)):\n",
    "        labelDict[i] = labels[i]\n",
    "    return labelDict\n",
    "\n",
    "label_indices = create_label_indices(df)\n",
    "print(label_indices)"
   ],
   "outputs": [],
   "metadata": {
    "id": "TWP1X17N5tJx"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#@title Define functions for under-/oversample dataset {display-mode: \"form\"}\n",
    "def undersample(df_trn, major_label, minor_label):\n",
    "  sample_size = sum(df_trn[config_data.label_column] == minor_label)\n",
    "  majority_indices = df_trn[df_trn[config_data.label_column] == major_label].index\n",
    "  random_indices = np.random.choice(majority_indices, sample_size, replace=False)\n",
    "  sample = df_trn.loc[random_indices]\n",
    "  sample = sample.append(df_trn[df_trn[config_data.label_column] == minor_label])\n",
    "  df_trn = sample\n",
    "  df_trn = df_trn.sample(frac=1, axis=0, random_state = config.seed)\n",
    "  print(df_trn[config_data.label_column].value_counts())\n",
    "  return df_trn\n",
    "\n",
    "def oversample(df_trn, major_label, minor_label):\n",
    "  minor_size = sum(df_trn[config_data.label_column] == minor_label)\n",
    "  major_size = sum(df_trn[config_data.label_column] == major_label)\n",
    "  multiplier = major_size//minor_size\n",
    "  sample = df_trn\n",
    "  minority_indices = df_trn[df_trn[config_data.label_column] == minor_label].index\n",
    "  diff = major_size - (multiplier * minor_size)     \n",
    "  random_indices = np.random.choice(minority_indices, diff, replace=False)\n",
    "  sample = pd.concat([df_trn.loc[random_indices], sample], ignore_index=True)\n",
    "  for i in range(multiplier - 1):\n",
    "    sample = pd.concat([sample, df_trn[df_trn[config_data.label_column] == minor_label]], ignore_index=True)\n",
    "  df_trn = sample\n",
    "  df_trn = df_trn.sample(frac=1, axis=0, random_state = config.seed)\n",
    "  print(df_trn[config_data.label_column].value_counts())\n",
    "  return df_trn"
   ],
   "outputs": [],
   "metadata": {
    "id": "wsfolsicu4tn"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#@title Function to split dataframe according to Sampling strategy and train size {display-mode: \"form\"}\n",
    "def split_dataframe(df, train_size = 0.8, random_state = None):\n",
    "    # split data into training and validation set\n",
    "    df_trn, df_valid = train_test_split(df, stratify = df[config_data.label_column], train_size = train_size, random_state = random_state)\n",
    "    # apply sample strategy\n",
    "    sizeOne = sum(df_trn[config_data.label_column] == 1)\n",
    "    sizeZero = sum(df_trn[config_data.label_column] == 0)\n",
    "    major_label = 0\n",
    "    minor_label = 1\n",
    "    if sizeOne > sizeZero:\n",
    "      major_label = 1\n",
    "      minor_label = 0\n",
    "    if config.sampling == Sampling.UnderSampling:\n",
    "      df_trn = undersample(df_trn, major_label, minor_label)\n",
    "    elif config.sampling == Sampling.OverSampling:\n",
    "      df_trn = oversample(df_trn, major_label, minor_label)\n",
    "    return df_trn, df_valid"
   ],
   "outputs": [],
   "metadata": {
    "id": "wg9tHTpplBVr"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Predictor\n"
   ],
   "metadata": {
    "id": "SClv488eQC8B"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#@title Create a predictor class{display-mode: \"form\"}\n",
    "class Predictor:\n",
    "    def __init__(self, classifier):\n",
    "        self.classifier = classifier\n",
    "        self.classes = self.classifier.data.classes\n",
    "\n",
    "    def predict(self, text):\n",
    "        prediction = self.classifier.predict(text)\n",
    "        prediction_class = prediction[1]\n",
    "        return self.classes[prediction_class]  "
   ],
   "outputs": [],
   "metadata": {
    "id": "qubb_Ka-C78O"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create and train the learner/classifier\n"
   ],
   "metadata": {
    "id": "zyVQS13d5Sft"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#@title Define functions to create databunch, learner and actual classifier{display-mode: \"form\"}\n",
    "def create_databunch(config, df_trn, df_valid):\n",
    "    bert_tok = BertTokenizer.from_pretrained(config.model_name,)\n",
    "    fastai_tokenizer = Tokenizer(tok_func=FastAiBertTokenizer(bert_tok, max_seq_len=config.max_seq_len), pre_rules=[], post_rules=[])\n",
    "    fastai_bert_vocab = Vocab(list(bert_tok.vocab.keys()))\n",
    "    return BertDataBunch.from_df(\".\", \n",
    "                   train_df=df_trn,\n",
    "                   valid_df=df_valid,\n",
    "                   tokenizer=fastai_tokenizer,\n",
    "                   vocab=fastai_bert_vocab,\n",
    "                   bs=config.bs,\n",
    "                   text_cols=input_col,\n",
    "                   label_cols=config_data.label_column,\n",
    "                   collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n",
    "              )\n",
    "\n",
    "\n",
    "def create_learner(config, databunch):\n",
    "    model = BertTextClassifier(config.model_name, config.num_labels)\n",
    "\n",
    "    optimizer = partial(AdamW)\n",
    "    if config.es:\n",
    "      learner = Learner(\n",
    "        databunch, model,\n",
    "        optimizer,\n",
    "        wd = config.weight_decay,\n",
    "        metrics=FBeta(beta=1), #accuracy, (metric to optimize on)\n",
    "        loss_func=config.loss_func, callback_fns=[partial(EarlyStoppingCallback, monitor='f_beta', min_delta=config.min_delta, patience=config.patience)]\n",
    "      )\n",
    "    else:\n",
    "      learner = Learner(\n",
    "        databunch, model,\n",
    "        optimizer,\n",
    "        wd = config.weight_decay,\n",
    "        metrics=FBeta(beta=1), #accuracy, (metric to optimize on)\n",
    "        loss_func=config.loss_func,\n",
    "      )\n",
    "    \n",
    "    return learner\n",
    "\n",
    "# Create the classifier\n",
    "def create_classifier(config, df):\n",
    "  df_trn, df_valid = split_dataframe(df, train_size = config.train_size, random_state = config.seed)\n",
    "  databunch = create_databunch(config, df_trn, df_valid)\n",
    "\n",
    "  return create_learner(config, databunch)"
   ],
   "outputs": [],
   "metadata": {
    "id": "T83UogVz5XJJ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#@title Define predict loop {display-mode: \"form\"}\n",
    "def predict_and_log_result(classifier, df_eval):\n",
    "  predictor = Predictor(classifier)\n",
    "  flat_predictions, flat_true_labels = [], []\n",
    "  column_index = df_eval.columns.get_loc(config_data.label_column)\n",
    "  for row in progress_bar(df_eval.itertuples(), total=len(df_eval)):\n",
    "      class_text = row.RequirementText\n",
    "      class_label = row[column_index+1]\n",
    "      flat_true_labels.append(class_label)\n",
    "      prediction = predictor.predict(class_text)\n",
    "      flat_predictions.append(prediction)\n",
    "\n",
    "      log_text = 'PID: {}, {}, {} -> {}'.format(row.ProjectID, class_text, label_indices.get(class_label), label_indices.get(prediction))\n",
    "      logLine(log_text)\n",
    "  \n",
    "  # get labels in correct order\n",
    "  target_names = []\n",
    "  test_labels = unique_labels(flat_true_labels, flat_predictions) \n",
    "  test_labels = np.sort(test_labels)\n",
    "  for x in test_labels:\n",
    "    target_names.append(label_indices.get(x))\n",
    "\n",
    "  result = classification_report(flat_true_labels, flat_predictions, target_names=target_names, digits = 5)\n",
    "  logResult(result)\n",
    "  print(result)\n",
    "  return flat_predictions, flat_true_labels"
   ],
   "outputs": [],
   "metadata": {
    "id": "iY9hh5FPo20_"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#@title Define train and test loop{display-mode: \"form\"}\n",
    "def train_and_predict(df_train, df_eval, overall_flat_predictions, overall_flat_true_labels, results):\n",
    "  classifier = create_classifier(config, df_train)\n",
    "  # Train the classifier on train set\n",
    "  print(classifier.fit_one_cycle(config.epochs, max_lr=config.max_lr, moms=config.moms, wd=config.weight_decay))\n",
    "  #Predict on test set\n",
    "  flat_predictions, flat_true_labels = predict_and_log_result(classifier, df_eval)\n",
    "  overall_flat_predictions.extend(flat_predictions)\n",
    "  overall_flat_true_labels.extend(flat_true_labels)\n",
    "  test_labels = df_eval[config_data.label_column].unique()\n",
    "  test_labels = np.sort(test_labels)\n",
    "  results.extend(precision_recall_fscore_support(flat_true_labels, flat_predictions, labels = test_labels))\n",
    "  return classifier, overall_flat_predictions, overall_flat_true_labels, results"
   ],
   "outputs": [],
   "metadata": {
    "id": "IZBH8aOZ1QfV"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#@title Decide how to fold and train the classifier {display-mode: \"form\"}\n",
    "overall_flat_predictions, overall_flat_true_labels, results = [], [], []\n",
    "initLog()\n",
    "\n",
    "if config.fold == Fold.TenFold: # Use Stratified ten fold cross validation\n",
    "  skf = StratifiedKFold(n_splits=10)\n",
    "  fold_number = 1\n",
    "  for train, test in skf.split(df, df[config_data.label_column]):\n",
    "    df_train = df.iloc[train]\n",
    "    df_eval = df.iloc[test]\n",
    "    log_text = '/////////////////////// Fold: {} of {} /////////////////////////////'.format(fold_number,10)\n",
    "    logLine(log_text)\n",
    "    classifier, overall_flat_predictions, overall_flat_true_labels, results = train_and_predict(df_train, df_eval, overall_flat_predictions, overall_flat_true_labels, results)\n",
    "    fold_number = fold_number + 1\n",
    "elif config.fold == Fold.ProjFold: # Use project specific fold as described in config_data\n",
    "  for k in config_data.project_fold:\n",
    "    test = df.loc[df['ProjectID'].isin(k)].index\n",
    "    train = df.loc[~df['ProjectID'].isin(k)].index\n",
    "    df_train = df.loc[train]\n",
    "    df_eval = df.loc[test]\n",
    "    log_text = '/////////////////////// Test-Projects: {} /////////////////////////////'.format(k)\n",
    "    logLine(log_text)\n",
    "    classifier, overall_flat_predictions, overall_flat_true_labels, results = train_and_predict(df_train, df_eval, overall_flat_predictions, overall_flat_true_labels, results)\n",
    "else: # Use train/test split\n",
    "  df_train, df_eval = train_test_split(df,stratify=df[config_data.label_column], train_size=config.train_size, random_state= config.seed)\n",
    "  classifier, overall_flat_predictions, overall_flat_true_labels, results = train_and_predict(df_train, df_eval, overall_flat_predictions, overall_flat_true_labels, results)\n",
    "\n",
    "get_memory_usage_str()\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "sqGyLEubBw60"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#@title Define function to calculate averaged metric results {display-mode: \"form\"}\n",
    "def calcAverageMetrics(results):\n",
    "  precisions, recalls, fscores = [], [], []\n",
    "  for i in range(int(len(results)/4)):\n",
    "    precisions.append(results[i*4])\n",
    "    recalls.append(results[i*4+1])\n",
    "    fscores.append(results[i*4+2])\n",
    "  precision = [0]*len(precisions[0])\n",
    "  recall = [0]*len(recalls[0])\n",
    "  fscore = [0]*len(fscores[0])\n",
    "  for i in range(len(precisions)):\n",
    "    precision = precision + precisions[i]\n",
    "    recall = recall + recalls[i]\n",
    "    fscore = fscore + fscores[i]\n",
    "  precision = precision / int(len(results)/4)\n",
    "  recall = recall / int(len(results)/4)\n",
    "  fscore = fscore / int(len(results)/4)\n",
    "  return precision, recall, fscore"
   ],
   "outputs": [],
   "metadata": {
    "id": "YPCIH1oPgCAf"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#@title Display and log overall evaluation results {display-mode: \"form\"}\n",
    "target_names = []\n",
    "test_labels = df_eval[config_data.label_column].unique()\n",
    "\n",
    "test_labels = np.sort(test_labels)\n",
    "for x in test_labels:\n",
    "  target_names.append(label_indices.get(x))\n",
    "\n",
    "print('/////////////////////// Aggregated Predictions Result /////////////////////////////')\n",
    "logResult('/////////////////////// Aggregated Predictions Result /////////////////////////////')\n",
    "result = classification_report(overall_flat_true_labels, overall_flat_predictions, target_names=target_names, digits = 5)\n",
    "logResult(result)\n",
    "print(result)\n",
    "print('/////////////////////// Averaged Metrics Result /////////////////////////////')\n",
    "logResult('/////////////////////// Averaged Metrics Result /////////////////////////////')\n",
    "precision, recall, fscore = calcAverageMetrics(results)\n",
    "print(\"              precision    recall  f1-score\")\n",
    "logResult(\"              precision    recall  f1-score\")\n",
    "for i in range(len(precision)):\n",
    "  print('{:<14}'.format(target_names[i]) + '  {:.5f}'.format(precision[i]) + '   {:.5f}'.format(recall[i]) + '   {:.5f}'.format(fscore[i]))\n",
    "  logResult('{:<14}'.format(target_names[i]) + '  {:.5f}'.format(precision[i]) + '   {:.5f}'.format(recall[i]) + '   {:.5f}'.format(fscore[i]))\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "5oGs3EQjnHu-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save Model"
   ],
   "metadata": {
    "id": "UeoCn0Bs-Wds"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#@title Save the model along with its config\n",
    "def create_model_name():\n",
    "    name = 'NoRBERT_{clasz}_e{epochs}_{sampling}'.format(clasz=clazz, epochs=str(config.epochs),sampling=Sampling(config.sampling).name)\n",
    "    return name\n",
    "\n",
    "def save_config(model_save_path, model_name):\n",
    "    settings = ''\n",
    "    for item in config.__dict__:\n",
    "        value = config[item]\n",
    "        setting = '{item}={value},\\n'.format(item=item, value=value)\n",
    "        settings += setting\n",
    "    save_path = model_save_path + model_name + '.config'\n",
    "    with open(save_path, 'w', encoding='utf-8') as out:\n",
    "        out.write(settings)\n",
    "\n",
    "if save_model:\n",
    "  model_name = create_model_name()\n",
    "  model_save_path = config_data.root_folder + config_data.model_path\n",
    "  if not os.path.isdir(model_save_path):\n",
    "    print(\"Models folder does not exist, trying to create folder.\")\n",
    "    try:\n",
    "      os.mkdir(model_save_path)\n",
    "    except OSError:\n",
    "      print (\"Creation of the directory %s failed\" % model_save_path)\n",
    "    else:\n",
    "      print (\"Successfully created the directory %s\" % model_save_path)\n",
    "  save_config(model_save_path, model_name)\n",
    "  model_save_file = model_save_path + model_name + '.pkl'\n",
    "  classifier.export(file = model_save_file)"
   ],
   "outputs": [],
   "metadata": {
    "id": "DXTWGILJ4kJx",
    "cellView": "form"
   }
  }
 ]
}
